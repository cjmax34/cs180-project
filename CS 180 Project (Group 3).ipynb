{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJAbqk29jP5E"
      },
      "source": [
        "# **Predicting Stress Level By Training a Multilayer Perceptron**\n",
        "\n",
        "This machine learning project aims to predict the stress level being experienced by an individual given their sleep health and various lifestyle habits. The project uses a Multilayer Perceptron (MLP) as the machine learning algorithm. [Sleep Health Data](https://www.kaggle.com/datasets/imaginativecoder/sleep-health-data-sampledt) is the dataset used for the project, and is acquired from Kaggle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a course requirement for CS 180 (Artificial Intelligence) Course of the Department of Computer Science, College of Engineering, University of the Philippines, Diliman under the guidance of Carlo Raquel for A.Y. 2023-2024.\n",
        "\n",
        "- MAXIMO, Calvin James T.\n",
        "- MENDOZA, Janelle M.\n",
        "- MURILLO, Joana Marie V.\n",
        "\n",
        "The GitHub repository for this project can be accessed [here](https://github.com/cjmax34/cs180-project)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importing libraries\n",
        "\n",
        "First, we have to import the libraries needed for the project. The libraries imported here are for loading the dataset, and performing exploratory data analysis (EDA). Additional libraries for modeling, and metrics will be imported later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading the dataset\n",
        "\n",
        "Next, we will now load the dataset and store it in a variable named `dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Sleep_Data_Sampled.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Performing Exploratory Data Analysis (EDA)\n",
        "\n",
        "Now we will perform EDA on our dataset. This is important to help us understand the dataset more before we make assumptions. There might be patterns, trends, and relationships in the dataset that may not be visible at first glance. There might also be rows that contain missing values, outliers, inconsistencies, or biases, which could lead to inaccurate results. By gaining a deeper understanding of the dataset at hand through performing EDA, we are able to choose the appropriate techniques and approaches in training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Shape of dataset\n",
        "\n",
        "We want to know the shape of the dataset, or the number of rows and columns it has. We can easily do this by calling `shape()` on `dataset`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prints (# of rows, # of columns)\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our dataset has 15000 rows (!!) and 13 columns/features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Getting information about the dataset\n",
        "\n",
        "We want more information about the dataset. We can do this by calling `info()` on `dataset`. This provides us essential information such as the number of columns, column names, and data types of each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provides information about the dataset\n",
        "dataset.info(); "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print column names of dataset\n",
        "dataset.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are 13 columns in total, namely: Person ID, Gender, Age, Occupation, Sleep Duration, Quality of Sleep, Physical Activity Level, Stress Level, BMI Category, Blood Pressure, Heart Rate, Daily Steps, and Sleep Disorder. \n",
        "\n",
        "The data types across all columns are float, int, and object (string)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Unique values per column\n",
        "\n",
        "In this part, we want to identify the unique values for each of the dataset's columns/features. This helps us ensure the consistent formatting of the values across all columms and assess the complexity of each categorical feature (number of unique values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of unique values per column\n",
        "dataset.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of unique Person IDs is 15000, which makes sense because it is used to identify an individual in the dataset. We will deal with this later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unique values per categorical feature\n",
        "for col in dataset.select_dtypes(include='object').columns:\n",
        "    print(f\"{col}: {list(dataset[col].unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the output above, it seems that the formatting of each column/feature's values is consistent. However, we have to preprocess some of the values, such as \"Male\" and \"Female\" in the Gender category, \"Normal Weight\" in the BMI Category feature, and the systolic and diastolic blood pressure measurements in the Blood Pressure category. We will delve into this later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### Renaming of column values\n",
        "\n",
        "As seen from the output of the previous code block, the unique values of the `BMI Category` feature are `Normal Weight`, `Normal`, `Overweight`, and `Obese`. We want to change `Normal Weight` to `Normal` since they are equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename Normal Weight to Normal\n",
        "dataset['BMI Category'] = dataset['BMI Category'].str.replace('Normal Weight', 'Normal')\n",
        "dataset['BMI Category'].unique()    # Verify that there are only three unique values  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have successfully renamed all `Normal Weight` entries to `Normal`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Statistical summary\n",
        "\n",
        "The `describe()` method returns a statistical summary of the numerical features in the given dataset. For each column, it returns these information.\n",
        "\n",
        "count - The number of not-empty values.\\\n",
        "mean - The average (mean) value.\\\n",
        "std - The standard deviation.\\\n",
        "min - the minimum value.\\\n",
        "25% - The 25% percentile.\\\n",
        "50% - The 50% percentile.\\\n",
        "75% - The 75% percentile.\\\n",
        "max - the maximum value.\n",
        "\n",
        "Reference: https://www.w3schools.com/python/pandas/ref_df_describe.asp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the table above that all numerical features have the same count (15000).\n",
        "\n",
        "The average age is 44.13 years. The average sleep duration is 7 hours. The average quality of sleep (1-10, 1 being the lowest and 10 being the highest) is 7.13. The average amount of daily physical activity is 59.93 minutes, or nearly an hour. The average stress level (1-10, 1 being the lowest and 10 being the highest) is 5.65. The average heart rate is 70.86 beats per minute. The average number of daily steps is 6795."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Checking for the presence of null values\n",
        "\n",
        "An important part of data exploration is checking for the presence of null values in the dataset. It is vital to handle null values because they can produce inaccurate or misleading results. Some machine learning algorithms, especially MLP, also cannot handle null values. Ultimately, it helps us in deciding the best way to handle the null values through various methods such as imputation and removal of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking for null values\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fortunately, there are no null values across **ALL** rows of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data visualization\n",
        "\n",
        "Data visualization is another important part of every machine learning project. In this part, we will be using graphs to better understand the patterns and trends in the data. These patterns and trends can easily be understood through visualization especially if they are hard to discern from the raw data. It condenses quite complex information into an easily digestible format. We can also discover what features correlate strongly with the target variable (stress level). Ultimately, it is a powerful tool for effectively communicating insights on data to a wider audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of stress level\n",
        "\n",
        "We want to visualize the distribution of the stress level feature, which is our target variable, in our dataset. The code block below is from [Tanaya Tipre's project](https://www.kaggle.com/code/tanayatipre/stress-level-detection#5.-Data-Visualization), with the labels modified for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of stress level\n",
        "sns.countplot(x='Stress Level', data=dataset)\n",
        "\n",
        "plt.xlabel('Stress Level')\n",
        "\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.title('Distribution of Stress Level')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the figure above, we can observe that the stress level is not distributed evenly, and a stress level of 6 is found across many records in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of numerical features\n",
        "\n",
        "We want to visualize the distribution of the numerical features of the dataset. We might be able to identify outliers and potential patterns across these features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of features to plot histograms for\n",
        "features = [col for col in dataset.columns if col not in ['Stress Level', 'Person ID']]\n",
        "\n",
        "# Plot histograms for each feature\n",
        "dataset.hist(column=features, bins=10, figsize=(10, 10))\n",
        "plt.suptitle(\"Histograms of Features\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of categorical features\n",
        "\n",
        "Visualizing the distribution of stress levels by Gender, BMI Category, and Sleep Disorder allows for a visual examination of trends and relationships in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(x='Gender', y='Stress Level', data=dataset)\n",
        "plt.title('Distribution of Stress Levels by Gender')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(x='BMI Category', y='Stress Level', data=dataset)\n",
        "plt.title('Distribution of Stress Levels by BMI Category')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(x='Sleep Disorder', y='Stress Level', data=dataset)\n",
        "plt.title('Distribution of Stress Levels by Sleep Disorder')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Solving and visualizing the correlation matrix\n",
        "\n",
        "We want to identify relationships among the variables and select features that are highly correlated. This helps in avoiding redundancy and building more accurate models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_features = dataset.select_dtypes(include=['number'])\n",
        "numerical_features.drop('Person ID', axis=1, inplace=True)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numerical_features.corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 10))\n",
        "sns.heatmap(correlation_matrix, cmap = 'crest', annot = True)\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Positively correlated columns\n",
        "\"Sleep Duration\" and \"Quality of Sleep\"\\\n",
        "\"Physical Activity Level\" and \"Daily Steps\"\\\n",
        "\"Stress Level\" and Heart Rate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Negatively correlated columns:\n",
        "\"Stress Level\" and \"Sleep Duration\"\\\n",
        "\"Stress Level\" and \"Quality of Sleep\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### No correlation with target column (stress level)\n",
        "\"Physical Activity Level\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data preprocessing\n",
        "\n",
        "Now that we have uncovered the possible trends and patterns in the dataset, we will now perform preprocessing. This involves renaming of columns or column values (if applicable), dropping of unused features, normalization of numerical features, label encoding (converting categorical data to numerical data), and dealing with null values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing libraries for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the necessary libraries to conduct preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are categorical features in the dataset such as `Gender`, `Occupation`, `BMI Category`, `Sleep Disorder`, and also the target variable `Stress Level`. Label encoding is utilized here to convert categorical values into numerical values by assigning a unique numerical label for each categorical value. This is done by importing `LabelEncoder`.\n",
        "\n",
        "On the other hand, standardizing numerical features such as `Age`, `Sleep Duration`, `Physical Activity Level`, `Blood Pressure`, `Heart Rate`, and `Daily Steps` is essential. This process ensures that variables with different units and scales contribute equally to the model, preventing biases and allowing machine learning algorithms to effectively learn patterns, leading to improved model performance.\n",
        "\n",
        "Reference: https://www.kaggle.com/code/alnourabdalrahman9/sleeping-disorders-detection-outliers-removal#Splitting-the-Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting blood pressure\n",
        "\n",
        "The `Blood Pressure` feature in our dataset is currently in the format of systolic pressure/diastolic pressure. We want to split them up such that there are separate columns for the systolic pressure and diastolic pressure to avoid inconsistencies when training the MLP.\n",
        "\n",
        "Systolic pressure is the maximum blood pressure during contraction of the ventricles; diastolic pressure is the minimum pressure recorded just prior to the next contraction.\n",
        "\n",
        "Reference: https://www.ncbi.nlm.nih.gov/books/NBK268/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the blood pressure feature into systolic blood pressure and diastolic blood pressure\n",
        "dataset[['Systolic Blood Pressure', 'Diastolic Blood Pressure']] = dataset['Blood Pressure'].str.split('/', n=1, expand=True)\n",
        "\n",
        "# Convert the data type of the newly created columns to numeric\n",
        "dataset[['Systolic Blood Pressure', 'Diastolic Blood Pressure']] = dataset[['Systolic Blood Pressure', 'Diastolic Blood Pressure']].apply(pd.to_numeric)\n",
        "\n",
        "# Verify that the data type conversion is successful\n",
        "print(dataset[['Systolic Blood Pressure', 'Diastolic Blood Pressure']].dtypes) \n",
        "dataset.head() # Verify that the splitting and dropping is successful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the output of the code block above that the splitting of blood pressure data into systolic and diastolic blood pressures was successful. We also dropped the blood pressure column, which can be observed in the table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting stress level as the last column\n",
        "\n",
        "To streamline the process of getting the training and testing data later, we will set the `Stress Level` feature as the last column of `dataset`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move the target column (stress level) to the last column\n",
        "dataset['Stress Level'] = dataset.pop('Stress Level')\n",
        "dataset.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropping of unnecessary features\n",
        "\n",
        "Some of the features in the dataset were deemed unnecessary and will be dropped/removed. Recall that we want to predict the stress level of an individual given their sleep health and lifestyle habits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove some features because they are unnecessary for analysis\n",
        "features_to_remove = ['Person ID', 'Occupation', 'Blood Pressure']\n",
        "\n",
        "dataset.drop(features_to_remove, axis=1, inplace=True)\n",
        "dataset.head()  # Verify that the features in the features_to_remove list have been dropped from the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Label encoding\n",
        "\n",
        "Now we will convert the categorical features (`Gender`, `BMI Category`, `Sleep Disorder`) to numerical data through the use of the `LabelEncoder` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()  # Instantiate the label encoder\n",
        "\n",
        "categorical_features = dataset.select_dtypes(include=['object']).columns.tolist()\n",
        "for cat in categorical_features:\n",
        "    dataset[cat] = label_encoder.fit_transform(dataset[cat])\n",
        "\n",
        "dataset.head()  # Verify that the categorical features have been encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Importing methods needed for training, modelling, and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For generating the training and testing sets (80% training, 20% testing)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For evaluating the model's performance\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# For the main model (Multilayer Perceptron)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# For the other models that will be used for comparison\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generating the training and testing sets\n",
        "\n",
        "After performing EDA and preprocessing the dataset, we are now ready to generate the data that will be used in training the model. For this project, we will use 80-20 splits (80% training, 20% testing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initializing the features and target variables\n",
        "X = dataset.drop('Stress Level', axis=1)\n",
        "y = dataset['Stress Level']\n",
        "\n",
        "# Generating the training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n",
        "\n",
        "# Displaying the dimensions of the training and testing sets\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print X_train\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparing to other models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_predict = log_reg.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matr = confusion_matrix(y_test, y_predict)\n",
        "print(\"Confusion Matrix : \\n\", conf_matr)\n",
        "\n",
        "classif_rep = classification_report(y_test, y_predict)\n",
        "print(\"Classification Report:\\n\", classif_rep)\n",
        "\n",
        "# Printing the test accuracy\n",
        "print(\"The test accuracy of Logistic Regression is : \", accuracy_score(y_test, y_predict) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a naive bayes model\n",
        "naive_bayes = GaussianNB()\n",
        "\n",
        "# Fit the model to the training data\n",
        "naive_bayes.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_predict = naive_bayes.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrx = confusion_matrix(y_test, y_predict)\n",
        "print(\"Confusion Matrix: \\n\", conf_matr)\n",
        "\n",
        "classif_rep = classification_report(y_test, y_predict)\n",
        "print(\"Classification Report: \\n\", classif_rep)\n",
        "\n",
        "# Printing the test accuracy\n",
        "print(\"The test accuracy of Naive Bayes is : \", naive_bayes.score(X_test,y_test) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Support Vector Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an svm model\n",
        "svm_classifier = svm.SVC(kernel='linear')\n",
        "\n",
        "# Fit the model to the training data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_predict = svm_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_predict)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrx = confusion_matrix(y_test, y_predict)\n",
        "print(\"Confusion Matrix: \\n\", conf_matr)\n",
        "\n",
        "classif_rep = classification_report(y_test, y_predict)\n",
        "print(\"Classification Report: \\n\", classif_rep)\n",
        "\n",
        "# Printing the test accuracy\n",
        "print(\"The test accuracy of SVM is : \", accuracy * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create knn model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Fit the model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_predict = knn.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrx = confusion_matrix(y_test, y_predict)\n",
        "print(\"Confusion Matrix: \\n\", conf_matr)\n",
        "\n",
        "classif_rep = classification_report(y_test, y_predict)\n",
        "print(\"Classification Report: \\n\", classif_rep)\n",
        "\n",
        "# Printing the test accuracy\n",
        "print(\"The test accuracy of Naive Bayes is : \", knn.score(X_test,y_test) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a random forest classifier model\n",
        "random_forest = RandomForestClassifier(n_estimators=13)\n",
        "\n",
        "# Fit the model to the training data\n",
        "random_forest.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_predict = random_forest.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrx = confusion_matrix(y_test, y_predict)\n",
        "print(\"Confusion Matrix: \\n\", conf_matr)\n",
        "\n",
        "classif_rep = classification_report(y_test, y_predict)\n",
        "print(\"Classification Report: \\n\", classif_rep)\n",
        "\n",
        "# Printing the test accuracy\n",
        "print(\"The test accuracy of Random Forest Classifier is : \", random_forest.score(X_test,y_test) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a decision model\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model to the training data\n",
        "decision_tree.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_predict = decision_tree.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrx = confusion_matrix(y_test, y_predict)\n",
        "print(\"Confusion Matrix: \\n\", conf_matr)\n",
        "\n",
        "classif_rep = classification_report(y_test, y_predict)\n",
        "print(\"Classification Report: \\n\", classif_rep)\n",
        "\n",
        "# Printing the test accuracy\n",
        "print(\"The test accuracy of Decision Tree is : \", decision_tree.score(X_test,y_test) * 100, \"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
