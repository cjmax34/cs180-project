{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJAbqk29jP5E"
      },
      "source": [
        "# **Predicting Stress Level By Training a Multilayer Perceptron**\n",
        "\n",
        "This machine learning project aims to predict the stress level being experienced by an individual given their sleep health and various lifestyle habits. The project uses a Multilayer Perceptron (MLP) as the machine learning algorithm. [Sleep Health Data](https://www.kaggle.com/datasets/imaginativecoder/sleep-health-data-sampledt) is the dataset used for the project, and is acquired from Kaggle. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a course requirement for CS 180 (Artificial Intelligence) Course of the Department of Computer Science, College of Engineering, University of the Philippines, Diliman under the guidance of Carlo Raquel for A.Y. 2023-2024.\n",
        "\n",
        "- MAXIMO, Calvin James T.\n",
        "- MENDOZA, Janelle M.\n",
        "- MURILLO, Joana Marie V.\n",
        "\n",
        "The GitHub repository for this project can be accessed [here](https://github.com/cjmax34/cs180-project)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importing libraries\n",
        "\n",
        "First, we have to import the libraries needed for the project. The libraries imported here are for loading the dataset, and performing exploratory data analysis (EDA). Additional libraries for modeling, and metrics will be imported later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading the dataset\n",
        "\n",
        "Next, we will now load the dataset and store it in a variable named `dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('Sleep_Data_Sampled.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Performing Exploratory Data Analysis (EDA)\n",
        "\n",
        "Now we will perform EDA on our dataset. This is important to help us understand the dataset more before we make assumptions. There might be patterns, trends, and relationships in the dataset that may not be visible at first glance. There might also be rows that contain missing values, outliers, inconsistencies, or biases, which could lead to inaccurate results. By gaining a deeper understanding of the dataset at hand through performing EDA, we are able to choose the appropriate techniques and approaches in training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Shape of dataset\n",
        "\n",
        "We want to know the shape of the dataset, or the number of rows and columns it has. We can easily do this by calling `shape()` on `dataset`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prints (# of rows, # of columns)\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our dataset has 15000 rows (!!) and 13 columns/features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Getting information about the dataset\n",
        "\n",
        "We want more information about the dataset. We can do this by calling `info()` on `dataset`. This provides us essential information such as the number of columns, column names, and data types of each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provides information about the dataset\n",
        "dataset.info(); "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print column names of dataset\n",
        "dataset.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are 13 columns in total, namely: Person ID, Gender, Age, Occupation, Sleep Duration, Quality of Sleep, Physical Activity Level, Stress Level, BMI Category, Blood Pressure, Heart Rate, Daily Steps, and Sleep Disorder. \n",
        "\n",
        "The data types across all columns are float, int, and object (string)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Unique values per column\n",
        "\n",
        "In this part, we want to identify the unique values for each of the dataset's columns/features. This helps us ensure the consistent formatting of the values across all columms and assess the complexity of each categorical feature (number of unique values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of unique values per column\n",
        "dataset.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of unique Person IDs is 15000, which makes sense because it is used to identify an individual in the dataset. We will deal with this later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unique values per categorical feature\n",
        "for col in dataset.select_dtypes(include='object').columns:\n",
        "    print(f\"{col}: {list(dataset[col].unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the output above, it seems that the formatting of each column/feature's values is consistent. However, we have to preprocess some of the values, such as \"Male\" and \"Female\" in the Gender category, \"Normal Weight\" in the BMI Category feature, and the systolic and diastolic blood pressure measurements in the Blood Pressure category. We will delve into this later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Statistical summary\n",
        "\n",
        "The `describe()` method returns a statistical summary of the numerical features in the given dataset. For each column, it returns these information.\n",
        "\n",
        "count - The number of not-empty values.\\\n",
        "mean - The average (mean) value.\\\n",
        "std - The standard deviation.\\\n",
        "min - the minimum value.\\\n",
        "25% - The 25% percentile.\\\n",
        "50% - The 50% percentile.\\\n",
        "75% - The 75% percentile.\\\n",
        "max - the maximum value.\n",
        "\n",
        "Reference: https://www.w3schools.com/python/pandas/ref_df_describe.asp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the table above that all numerical features have the same count (15000).\n",
        "\n",
        "The average age is 44.13 years. The average sleep duration is 7 hours. The average quality of sleep (1-10, 1 being the lowest and 10 being the highest) is 7.13. The average amount of daily physical activity is 59.93 minutes, or nearly an hour. The average stress level (1-10, 1 being the lowest and 10 being the highest) is 5.65. The average heart rate is 70.86 beats per minute. The average number of daily steps is 6795."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Checking for the presence of null values\n",
        "\n",
        "An important part of data exploration is checking for the presence of null values in the dataset. It is vital to handle null values because they can produce inaccurate or misleading results. Some machine learning algorithms, especially MLP, also cannot handle null values. Ultimately, it helps us in deciding the best way to handle the null values through various methods such as imputation and removal of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checking for null values\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fortunately, there are no null values across **ALL** rows of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data visualization\n",
        "\n",
        "Data visualization is another important part of every machine learning project. In this part, we will be using graphs to better understand the patterns and trends in the data. These patterns and trends can easily be understood through visualization especially if they are hard to discern from the raw data. It condenses quite complex information into an easily digestible format. We can also discover what features correlate strongly with the target variable (stress level). Ultimately, it is a powerful tool for effectively communicating insights on data to a wider audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of stress level\n",
        "\n",
        "We want to visualize the distribution of the stress level feature, which is our target variable, in our dataset. The code block below is from [Tanaya Tipre's project](https://www.kaggle.com/code/tanayatipre/stress-level-detection#5.-Data-Visualization), with the labels modified for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of stress level\n",
        "sns.countplot(x='Stress Level', data=dataset)\n",
        "\n",
        "plt.xlabel('Stress Level')\n",
        "\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.title('Distribution of Stress Level')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the figure above, we can observe that the stress level is not distributed evenly, and a stress level of 6 is found across many records in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distribution of numerical features\n",
        "\n",
        "We want to visualize the distribution of the numerical features of the dataset. We might be able to identify outliers and potential patterns across these features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of features to plot histograms for\n",
        "features = [col for col in dataset.columns if col not in ['Stress Level', 'Person ID']]\n",
        "\n",
        "# Plot histograms for each feature\n",
        "dataset.hist(column=features, bins=10, figsize=(10, 10))\n",
        "plt.suptitle(\"Histograms of Features\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Insert explanation here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data preprocessing\n",
        "\n",
        "Now that we have uncovered the possible trends and patterns in the dataset, we will now perform preprocessing. This involves renaming of columns or column values (if applicable), dropping of unused features, normalization of numerical features, label encoding (converting categorical data to numerical data), and dealing with null values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing libraries for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the necessary libraries to conduct preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "label_encoder = LabelEncoder()  # Instantiate the label encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are categorical features in the dataset such as `Gender`, `Occupation`, `BMI Category`, `Sleep Disorder`, and also the target variable `Stress Level`. Label encoding is utilized here to convert categorical values into numerical values by assigning a unique numerical label for each categorical value. This is done by importing `LabelEncoder`.\n",
        "\n",
        "On the other hand, standardizing numerical features such as `Age`, `Sleep Duration`, `Physical Activity Level`, `Blood Pressure`, `Heart Rate`, and `Daily Steps` is essential. This process ensures that variables with different units and scales contribute equally to the model, preventing biases and allowing machine learning algorithms to effectively learn patterns, leading to improved model performance.\n",
        "\n",
        "Reference: https://www.kaggle.com/code/alnourabdalrahman9/sleeping-disorders-detection-outliers-removal#Splitting-the-Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropping of unnecessary features\n",
        "\n",
        "Some of the features in the dataset were deemed unnecessary and will be dropped/removed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove some features because they are unnecessary for analysis\n",
        "features_to_remove = ['Person ID', 'Occupation', 'Quality of Sleep','Physical Activity Level','Sleep Disorder']\n",
        "\n",
        "dataset.drop(features_to_remove, axis=1, inplace=True)\n",
        "dataset.head()  # Verify that the features in the features_to_remove list have been dropped from the dataset"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
